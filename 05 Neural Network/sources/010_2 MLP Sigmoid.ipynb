{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Bigvalue_Bigdata Lab\n",
    "\n",
    "작성자 : 구름\n",
    "목적 : 다층 퍼셉트론의 학습방법에 대한 이해를 위해 작성\n",
    "XOR연산을 구현하기위해 은닉노드 2개와 출력노드 1개로 구성된 다층 퍼셉트론 생성 후\n",
    "시그모이드 함수를 이용한 FeedForward와 Backpropagation 과정 학습\n",
    "반복 학습을 통한 네트워크 학습 과정 확인\n",
    "\n",
    "https://haningya.tistory.com/295\n",
    "소스 참고\n",
    "\"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# ds=[[0,0,0],[0,1,0],[1,0,0],[1,1,1]] #AND\n",
    "ds=[[0,0,0],[0,1,1],[1,0,1],[1,1,1]] #OR\n",
    "# ds=[[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 0]]  #NAND\n",
    "# ds=[[0,0,0],[0,1,1],[1,0,1],[1,1,0]] #XOR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"데이터 재구성\"\"\"\n",
    "nds = np.array(ds)  # 넘파이로 변환\n",
    "\n",
    "X = nds[:, :2].T.reshape(2, 4)  # x1,x2만 잘라내고 전치행렬로 뒤집어서 배열 재구성\n",
    "Y = nds[:, 2:].reshape(1, 4)  # y만 잘라내고 배열 재구성\n",
    "del nds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"초기 가중치와 바이어스 세팅 (고정)\"\"\"\n",
    "W1 = np.array([[0.07, 0.09], [0.01, 0.02]])\n",
    "B1 = np.array([[0.04], [0.09]])\n",
    "W2 = np.array([[0.07], [0.05]])\n",
    "B2 = np.array([[0.06]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"함수 선언\"\"\"\n",
    "\n",
    "\n",
    "# 입력값과 가중치를 곱한 후 바이어스와 함께 합함\n",
    "def affine(W, X, B):\n",
    "    return np.dot(W.T, X) + B\n",
    "\n",
    "\n",
    "# 시그모이드 함수 정의\n",
    "def sigmoid(o):\n",
    "    return 1. / (1 + np.exp(-1 * o))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"순전파 1회 진행\"\"\"\n",
    "# 첫번째 은닉층의 2개 은닉 노드에 데이터 전달\n",
    "Z1 = affine(W1, X, B1)\n",
    "H = sigmoid(Z1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 출력층 출력노드에 은닉층의 출력 정보를 전달하여 예측\n",
    "Z2 = affine(W2, H, B2)\n",
    "Y_hat = sigmoid(Z2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"손실함수 계산\"\"\"\n",
    "# MSE를 활용한 손실함수 계산\n",
    "error = Y - Y_hat\n",
    "squared_error = np.power(error, 2)\n",
    "loss = 1. / X.shape[1] * np.sum(squared_error)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" 역전파 학습 계산 출력 노드에 대한 편미분\"\"\"\n",
    "# 출력층 가중치 W2로 Loss 함수에 대한 편미분값을 연쇄법칙을 이용해 계산\n",
    "dW2 = np.dot(H, (-2 * (Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2))).T)\n",
    "\n",
    "# 출력층 바이어스(절편)으로 Loss 함수에 대한 편미분 값을 연쇄 법칙을 이용해 계산\n",
    "# 바이어스는 4개 값에 대해 동일하므로 평균 을 한번에 반영\n",
    "# keepdims는 2차원 배열 구조를 그대로 유지시키기 위해 사용\n",
    "dB2 = 1. / 4. * np.sum(-(Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2)), axis=1, keepdims=True)\n",
    "\n",
    "# 히든 노드의 학습\n",
    "dH = np.dot(W2, -(Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2)))\n",
    "\n",
    "# BackPropagate: Input Layer\n",
    "dZ1 = dH * H * (1 - H)\n",
    "dW1 = np.dot(X, dZ1.T)\n",
    "dB1 = 1. / 4. * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "dW1, dB1, dW2, dB2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" 학습률 정의 \"\"\"\n",
    "learning_rate = 0.5\n",
    "\n",
    "W1 += learning_rate * dW1\n",
    "B1 += learning_rate * dB1\n",
    "W2 += learning_rate * dW2\n",
    "B2 += learning_rate * dB2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"학습 결과 보기\"\"\"\n",
    "# 첫번째 은닉층의 2개 은닉 노드에 데이터 전달\n",
    "Z1 = affine(W1, X, B1)\n",
    "H = sigmoid(Z1)\n",
    "\n",
    "# 출력층 출력노드에 은닉층의 출력 정보를 전달하여 예측\n",
    "Z2 = affine(W2, H, B2)\n",
    "Y_hat = sigmoid(Z2)\n",
    "\n",
    "\"\"\"손실함수 계산\"\"\"\n",
    "# MSE를 활용한 손실함수 계산\n",
    "loss2 = 1. / X.shape[1] * np.sum(np.power(Y - Y_hat, 2))\n",
    "\n",
    "print(\"학습전 손실함수 : {0:.7f} // 합습후 손실함수 : {1:.7f}\".format(loss, loss2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"학습 루프를 돌기 위해 함수 구성\"\"\"\n",
    "\n",
    "\n",
    "def init_random_parameters(num_hidden=2, deviation=1):\n",
    "    W1 = np.random.rand(2, num_hidden) * deviation\n",
    "    B1 = np.random.random((num_hidden, 1)) * deviation\n",
    "    W2 = np.random.rand(num_hidden, 1) * deviation\n",
    "    B2 = np.random.random((1, 1)) * deviation\n",
    "    return W1, B1, W2, B2\n",
    "\n",
    "\n",
    "\"\"\"손실함수 계산\"\"\"\n",
    "\n",
    "\n",
    "def loss_eval(_params):\n",
    "    W1, B1, W2, B2 = _params\n",
    "\n",
    "    # Forward: input Layer\n",
    "    Z1 = affine(W1, X, B1)\n",
    "    H = sigmoid(Z1)\n",
    "\n",
    "    # Forward: Hidden Layer\n",
    "    Z2 = affine(W2, H, B2)\n",
    "    Y_hat = sigmoid(Z2)\n",
    "\n",
    "    loss = 1. / X.shape[1] * np.sum(np.power(Y - Y_hat, 2))  # MSE\n",
    "\n",
    "    return Z1, H, Z2, Y_hat, loss\n",
    "\n",
    "\n",
    "# loss_eval ([W1, B1, W2, B2])[-1]\n",
    "\n",
    "def get_gradients(_params):\n",
    "    W1, B1, W2, B2 = _params\n",
    "    m = X.shape[1]\n",
    "\n",
    "    Z1, H, Z2, Y_hat, loss = loss_eval([W1, B1, W2, B2])\n",
    "\n",
    "    # BackPropagate: Hidden Layer\n",
    "    dW2 = np.dot(H, (-2 * (Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2))).T)  # MSE\n",
    "\n",
    "    dB2 = 1. / 4. * np.sum(-(Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2)), axis=1, keepdims=True)\n",
    "\n",
    "    dH = np.dot(W2, -(Y - Y_hat) * sigmoid(Z2) * (1 - sigmoid(Z2)))\n",
    "\n",
    "    # BackPropagate: Input Layer\n",
    "    dZ1 = dH * H * (1 - H)\n",
    "    dW1 = np.dot(X, dZ1.T)\n",
    "    dB1 = 1. / 4. * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return [dW1, dB1, dW2, dB2], loss\n",
    "\n",
    "\n",
    "def printProgress(iteration, total, prefix='', suffix='', decimals=1, barLength=100):\n",
    "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\n",
    "    percent = formatStr.format(100 * (iteration / float(total)))\n",
    "    filledLength = int(round(barLength * iteration / float(total)))\n",
    "    bar = '#' * filledLength + '-' * (barLength - filledLength)\n",
    "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\n",
    "    if iteration == total:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def optimize(_params, learning_rate=0.1, iteration=1000, sample_size=0):\n",
    "    params = np.copy(_params)\n",
    "\n",
    "    loss_trace = []\n",
    "\n",
    "    for epoch in range(iteration):\n",
    "\n",
    "        dparams, loss = get_gradients(params)\n",
    "\n",
    "        for param, dparam in zip(params, dparams):\n",
    "            param += - learning_rate * dparam\n",
    "\n",
    "        if (epoch % 1000 == 0):\n",
    "            loss_trace.append(loss)\n",
    "            printProgress(epoch, iteration, '학습진행', 'loss = ' + str(loss), 1, 20)\n",
    "\n",
    "    _, _, _, Y_hat_predict, _ = loss_eval(params)\n",
    "\n",
    "    return params, loss_trace, Y_hat_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"실제 학습 진행\"\"\"\n",
    "[W1, B1, W2, B2] = init_random_parameters()\n",
    "new_params, loss_trace, Y_hat_predict = optimize([W1, B1, W2, B2], learning_rate, 100000)\n",
    "\n",
    "# Plot learning curve (with costs)\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.show()\n",
    "\n",
    "print(Y_hat_predict)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}